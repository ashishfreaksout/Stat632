---
title: 'STAT 632: Homework 1'
author: 'Ashish Ashish'
date: 'Due: Feb 7th, 2025 at 11:59pm'
output:
  pdf_document: default
  html_document: default
  latex_engine: xelatex
  word_document: default
---

```{r, results="hide" , include=FALSE}
# install.packages("pacman)
pacman::p_load(ggplot2, rlang, dplyr, stringr, maps, mapdata, viridis, mapproj, tidyverse)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

\textbf{Excercise 0 :} link to Github. 
https://github.com/ashishfreaksout/Stat632

\section*{Concept Questions}
\textbf{Excercise 1 :}
\subsection*{(a) Least Squares Regression Line}
The equation for the least squares regression line is given by:
\[ \hat{y} = b_0 + b_1 x \]
From the regression summary:
\begin{align*}
    b_0 &= -1.1016 \\
    b_1 &= 2.2606
\end{align*}
Thus, the regression equation is:
\[ \hat{y} = -1.1016 + 2.2606x \]

\subsection*{(b) Hypothesis Test for the Slope}
The hypotheses for testing whether the slope is significantly different from zero are:
\begin{align*}
    H_0 &: \beta_1 = 0 \quad \text{(No relationship between } x \text{ and } y\text{)} \\
    H_A &: \beta_1 \neq 0 \quad \text{(There is a relationship between } x \text{ and } y\text{)}
\end{align*}
The p-value for the slope is \(< 2e-16\), which is extremely small. Since this is far below the typical significance level (\(\alpha = 0.05\)), we \textbf{reject the null hypothesis} and conclude that the slope is significantly different from zero.

\subsection*{(c) Missing p-value for the Intercept}
The p-value is calculated using the t-statistic formula:
```{r}
t_statistic <- -2.699  # t-value for the intercept
p_value <- 2 * pt(t_statistic, df = 50 - 2)# Compute the two-tailed p-value
p_value

```

missing p-value is 0.0095
\subsection*{(d) Missing t-statistic for the Slope}
for the slope:
\[ t = \frac{2.2606}{0.0981} = 23.048 \]

\subsection*{(e) 95\% Confidence Interval for the Slope}
A confidence interval for the slope is given by:
\[ b_1 \pm t^* \cdot SE(b_1) \]
where:
\begin{itemize}
    \item \( t^* \) is the critical value from the \( t \)-distribution with \( df = 50-2 = 48 \). For a 95\% confidence level, \( t^* \) is:
\end{itemize}
```{r}
tcrit <- qt(0.975, df=50-2) # value of tcritical
conf1 <- 2.2606 - 0.0981*tcrit #first conf interval
conf2 <- 2.2606 + 0.0981*tcrit #second conf interval
conf1
conf2
```

Since the confidence interval \((2.0633, 2.4579)\) \textbf{does not include 0}, it agrees with the hypothesis test’s conclusion that the slope is significantly different from zero.


\textbf{Excercise 2:}

Consider the linear regression model through the origin:
\begin{equation}
    Y_i = \beta x_i + e_i, \quad i = 1, \dots, n
\end{equation}
where the errors are independent and normally distributed:
\begin{equation}
    e_i \sim N(0, \sigma^2).
\end{equation}

\subsection*{(a) Finding the Least Squares Estimate of \( \beta \)}
To find the least squares estimate of \( \beta \), we minimize the residual sum of squares:
\begin{equation}
    R(\beta) = \sum_{i=1}^{n} (y_i - \beta x_i)^2.
\end{equation}
Taking the derivative with respect to \( \beta \) and setting it to zero:
\begin{equation}
    \frac{d}{d\beta} \sum_{i=1}^{n} (y_i - \beta x_i)^2 = \sum_{i=1}^{n} 2 (y_i - \beta x_i)(-x_i) = 0.
\end{equation}
Expanding and solving for \( \beta \):
\begin{equation}
    \sum_{i=1}^{n} x_i y_i - \beta \sum_{i=1}^{n} x_i^2 = 0.
\end{equation}
Thus, the least squares estimate of \( \beta \) is:
\begin{equation}
    \hat{\beta} = \frac{\sum_{i=1}^{n} x_i y_i}{\sum_{i=1}^{n} x_i^2}.
\end{equation}

\subsection*{(b) Expectation of \( \hat{\beta} \)}
Taking the expectation:
\begin{equation}
    E(\hat{\beta}) = E \left( \frac{\sum_{i=1}^{n} x_i y_i}{\sum_{i=1}^{n} x_i^2} \right).
\end{equation}
Substituting \( Y_i = \beta x_i + e_i \):
\begin{equation}
    E(\hat{\beta}) = E \left( \frac{\sum_{i=1}^{n} x_i (\beta x_i + e_i)}{\sum_{i=1}^{n} x_i^2} \right).
\end{equation}
Expanding the summation:
\begin{equation}
    E(\hat{\beta}) = \frac{\sum_{i=1}^{n} x_i \beta x_i + \sum_{i=1}^{n} x_i e_i}{\sum_{i=1}^{n} x_i^2}.
\end{equation}
Since \( E(e_i) = 0 \), the second summation vanishes:
\begin{equation}
    E(\hat{\beta}) = \frac{\beta \sum_{i=1}^{n} x_i^2}{\sum_{i=1}^{n} x_i^2} = \beta.
\end{equation}
Thus, \( \hat{\beta} \) is an unbiased estimator of \( \beta \).

\subsection*{(c) Variance of \( \hat{\beta} \)}
Using the variance property:
\begin{equation}
    \text{Var}(\hat{\beta}) = \text{Var} \left( \frac{\sum_{i=1}^{n} x_i y_i}{\sum_{i=1}^{n} x_i^2} \right).
\end{equation}
Substituting \( Y_i = \beta x_i + e_i \):
\begin{equation}
    \text{Var}(\hat{\beta}) = \text{Var} \left( \frac{\sum_{i=1}^{n} x_i (\beta x_i + e_i)}{\sum_{i=1}^{n} x_i^2} \right).
\end{equation}
Since variance only affects the error term:
\begin{equation}
    \text{Var}(\hat{\beta}) = \text{Var} \left( \frac{\sum_{i=1}^{n} x_i e_i}{\sum_{i=1}^{n} x_i^2} \right).
\end{equation}
Using the property that \( e_i \sim N(0, \sigma^2) \) and are independent:
\begin{equation}
    \text{Var}(\hat{\beta}) = \frac{\sum_{i=1}^{n} x_i^2 \sigma^2}{(\sum_{i=1}^{n} x_i^2)^2} = \frac{\sigma^2}{\sum_{i=1}^{n} x_i^2}.
\end{equation}
Thus, the variance of \( \hat{\beta} \) is:
\begin{equation}
    \text{Var}(\hat{\beta}) = \frac{\sigma^2}{\sum_{i=1}^{n} x_i^2}.
\end{equation}
\section*{Data Analysis Questions}
\textbf{Excercise 3:}

```{r,message=FALSE}
library(readr)
playbill <- read_csv("~/Downloads/playbill.csv")
head(playbill)
```
\subsection*{(a) Load the data, make a scatter plot, and fit the regression model}
```{r}
lm1 <- lm(CurrentWeek ~ LastWeek, data = playbill)
# Scatter plot with regression line
plot(CurrentWeek ~ LastWeek,data = playbill, 
     main = "Scatter plot of Current vs Last Week Gross Box Office",
     xlab = "Last Week Gross ($)", 
     ylab = "Current Week Gross ($)",
     pch = 16, col = "blue")
abline(lm1, col = "red")
```
    
\subsection*{(b) Compute 95\% confidence intervals for the intercept and slope}

```{r}
confint(lm1)
```

The 95\% confidence interval for the slope \( \beta_1 \) is:

\[
0.9515 \leq \beta_1 \leq 1.0127
\]

Since the value \( 1 \) falls within this confidence interval, it suggests that \( \beta_1 = 1 \) is a plausible value. This means that next week's gross box office revenue could reasonably be predicted using this week's revenue.

\subsection*{(c) Predict the gross box office for a show with $400,000 in the previous week}
```{r}
new_data <- data.frame(LastWeek = 400000)

# Estimate expected gross box office revenue
predict(lm1, new_data)

# Compute 95% prediction interval
predict(lm1, new_data, interval = "prediction", level = 0.95)

```
Using the fitted regression model, the estimated gross box office result for the current week is:

\[
\hat{Y} = 399637.5
\]

The 95\% prediction interval for the gross box office results in the current week is:

\[
(359832.8, 439442.2)
\]

Since \$450,000 is outside this prediction interval, it is unlikely (but not impossible) that a production with \$400,000 in the previous week’s gross box office will achieve \$450,000 in the current week.


\subsection*{(d) Evaluating the Prediction Rule: "Next Week’s Gross Box Office Equals This Week’s Gross Box Office"}

```{r}
#summary of the linear model
summary(lm1)
```
According to the summary, the R-squared value is 0.9966 that means the model is quite efficient for prediction. That means that promotors can use this model to predict the next week's earnings.

\textbf{Excercise 4:}
\subsection*{(a) Perform Simple Linear Regression}

```{r}
library(alr4)

# Fit the linear model
lm2 <- lm(Interval ~ Duration, data = oldfaith)
summary(lm2)
```
\subsection*{(b) Scatter Plot with Regression Line}
```{r}
# scatter plot of duration and interval
plot(Duration ~ Interval, data = oldfaith,
     main = "Scatter Plot of Interval vs. Duration", 
     xlab = "Duration (seconds)", 
     ylab = "Interval (minutes)", 
     pch = 16, col = "blue")
abline(lm2, col = "red")
```

\subsection*{(c) Compute 95\% confidence intervals }

```{r}
# new data frame with the given Duration
new_data <- data.frame(Duration = 250)

# Predict Interval with confidence and prediction intervals
prediction <- predict(lm2, newdata = new_data, interval = "prediction", level = 0.95)
prediction
```
this indicates that if an eruption lasts 250 seconds, the predicted waiting time until the next eruption is approximately 78.2 minutes.

Additionally, the 95% prediction interval for the waiting time is [66.35, 90.05] minutes. This means that, based on the regression model, we expect the actual waiting time to fall within this range 95% of the time for a new observation.

\subsection*{(d) $R^2$ Interpretation}

Multiple R-squared is   0.8029 , this means the model provides a good fit to the data, meaning that eruption duration is a strong predictor of waiting time. However, there is still some unexplained variation, suggesting that additional factors might influence the waiting time.



